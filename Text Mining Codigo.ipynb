{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de datos"
      ],
      "metadata": {
        "id": "DwlZ9euRYNRM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNn2BNd_VxQG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "df_true = pd.read_csv(\"True.csv\", engine=\"python\", on_bad_lines=\"skip\")\n",
        "df_fake = pd.read_csv(\"Fake.csv\", engine=\"python\", on_bad_lines=\"skip\")\n",
        "\n",
        "print(df_true.shape)\n",
        "print(df_fake.shape)\n",
        "\n",
        "df_true.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_true[\"label\"] = 1\n",
        "df_fake[\"label\"] = 0\n",
        "\n",
        "# Unimos datasets\n",
        "df = pd.concat([df_true, df_fake], axis=0)\n",
        "\n",
        "# Barajamos\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MWQ4qnpnXHFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "pBSk8ZUNXN-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"].value_counts()"
      ],
      "metadata": {
        "id": "ROl1v19NXVFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "gfqGL19aYVu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=\"label\", data=df)\n",
        "plt.title(\"Distribución de noticias Fake vs Real\")\n",
        "plt.xlabel(\"Label (0 = Fake, 1 = Real)\")\n",
        "plt.ylabel(\"Número de noticias\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sToDjbRsXayd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_length'] = df['text'].apply(len)\n",
        "\n",
        "# Número de palabras\n",
        "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "print(df[['text_length', 'word_count']].describe())\n",
        "\n",
        "# Visualizar histograma de longitud de palabras\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(df['word_count'], bins=50, kde=True)\n",
        "plt.title(\"Distribución del número de palabras por noticia\")\n",
        "plt.xlabel(\"Número de palabras\")\n",
        "plt.ylabel(\"Cantidad de noticias\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RVpDMBrAXjhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar textos vacíos\n",
        "df = df[df['word_count'] > 0].copy()\n",
        "\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "print(\"Nuevo tamaño del dataset:\", df.shape)"
      ],
      "metadata": {
        "id": "xmd0uMw7XqBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento"
      ],
      "metadata": {
        "id": "04VXYQZFYdXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "df['full_text'] = df['title'] + \" \" + df['text']\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # pasamos a minúsculas\n",
        "    text = re.sub(r'http\\S+','', text)  # quitamos URLs\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # quitamos puntuación y números\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df['clean_text'] = df['full_text'].apply(clean_text)\n",
        "df[['full_text','clean_text']].head()"
      ],
      "metadata": {
        "id": "aHHwOD3iXv3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tabla pero con mas caracteres\n",
        "example_df = df[['full_text','clean_text']].head(5).copy()\n",
        "example_df['full_text'] = example_df['full_text'].str[:100] + \"...\"\n",
        "example_df['clean_text'] = example_df['clean_text'].str[:100] + \"...\"\n",
        "\n",
        "display(example_df.style.set_caption(\"Tabla: Ejemplo resumido de textos originales y limpios\")\n",
        "        .set_properties(**{'text-align': 'left'}))"
      ],
      "metadata": {
        "id": "ExxCB8sm-i5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_df = df[['full_text','clean_text']].head(5).copy()\n",
        "example_df['full_text_len'] = example_df['full_text'].apply(lambda x: len(x.split()))\n",
        "example_df['clean_text_len'] = example_df['clean_text'].apply(lambda x: len(x.split()))\n",
        "example_df = example_df[['full_text_len','clean_text_len']]\n",
        "\n",
        "display(example_df.style.set_caption(\"Tabla 3: Longitud de textos originales y limpios (número de palabras)\")\n",
        "        .background_gradient(cmap=\"Blues\", axis=1))\n",
        "\n",
        "# Ejemplo antes vs despues\n",
        "for i, row in df[['full_text','clean_text']].head(3).iterrows():\n",
        "    print(f\"--- Ejemplo {i+1} ---\")\n",
        "    print(\"Original:\", row['full_text'][:200], \"...\")\n",
        "    print(\"Limpio  :\", row['clean_text'][:200], \"...\\n\")\n"
      ],
      "metadata": {
        "id": "D35zmjXJ_GBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=10000)\n",
        "X_tfidf = tfidf.fit_transform(df['clean_text'])\n",
        "\n",
        "y = df['label']\n",
        "\n",
        "print(\"Tamaño TF-IDF:\", X_tfidf.shape)"
      ],
      "metadata": {
        "id": "kWUKadptY-vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "A46d3pSJZJp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Creamos y entrenamos modelo\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predecir\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Matriz de confusión\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake','Real'], yticklabels=['Fake','Real'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Matriz de Confusión\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IPn-rhUUZKmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(clf, X_tfidf, y, cv=5, scoring='accuracy')\n",
        "print(\"Accuracy CV (5 folds):\", scores)\n",
        "print(\"Mean accuracy:\", np.mean(scores))"
      ],
      "metadata": {
        "id": "YSPwKEZEZr5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence transformers"
      ],
      "metadata": {
        "id": "iKtAJf76Z4ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('all-MiniLM-L12-v2')\n",
        "X_emb = model.encode(df['clean_text'], batch_size=64, show_progress_bar=True)\n",
        "\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_emb, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Entrenamos modelo\n",
        "clf_emb = LogisticRegression(max_iter=1000)\n",
        "clf_emb.fit(X_train, y_train)\n",
        "\n",
        "# Predecir\n",
        "y_pred = clf_emb.predict(X_test)\n",
        "\n",
        "print(\"Accuracy (embeddings):\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "rP9l4JgxZ6KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validación cruzada con embeddings"
      ],
      "metadata": {
        "id": "CozrIK5vbb81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic regression sobre embeddings\n",
        "clf_emb_cv = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# 5-fold cross-validation\n",
        "scores_emb = cross_val_score(clf_emb_cv, X_emb, y, cv=5, scoring='accuracy')\n",
        "print(\"Accuracy CV embeddings (5 folds):\", scores_emb)\n",
        "print(\"Mean accuracy embeddings:\", np.mean(scores_emb))"
      ],
      "metadata": {
        "id": "_yIZ7xlCbc7x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}